{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surface-cornwall",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aptmess/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aptmess/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/aptmess/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aptmess/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "# Corus - NLP datasets\n",
    "import corus\n",
    "from corus import load_lenta\n",
    "\n",
    "#NLTK - Natural Language Tool Kit\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk import ngrams\n",
    "\n",
    "#Other\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "roman-commander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<th>Dataset</th>\n",
       "<th>API <code>from corus import</code></th>\n",
       "<th>Tags</th>\n",
       "<th>Texts</th>\n",
       "<th>Uncompressed</th>\n",
       "<th>Description</th>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://github.com/yutkin/Lenta.Ru-News-Dataset\">Lenta.ru</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_lenta\"></a>\n",
       "<code><a href=\"#load_lenta\">load_lenta</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "739&nbsp;351\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1.66 Gb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://russe.nlpub.org/downloads/\">Lib.rus.ec</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_librusec\"></a>\n",
       "<code><a href=\"#load_librusec\">load_librusec</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>fiction</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "301&nbsp;871\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "144.92 Gb\n",
       "</td>\n",
       "<td>\n",
       "Dump of lib.rus.ec prepared for RUSSE workshop\n",
       "</br>\n",
       "</br>\n",
       "<code>wget http://panchenko.me/data/russe/librusec_fb2.plain.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://github.com/RossiyaSegodnya/ria_news_dataset\">Rossiya Segodnya</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ria_raw\"></a>\n",
       "<code><a href=\"#load_ria_raw\">load_ria_raw</a></code>\n",
       "</br>\n",
       "<a name=\"load_ria\"></a>\n",
       "<code><a href=\"#load_ria\">load_ria</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1&nbsp;003&nbsp;869\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "3.70 Gb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/RossiyaSegodnya/ria_news_dataset/raw/master/ria.json.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"http://study.mokoron.com/\">Mokoron Russian Twitter Corpus</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_mokoron\"></a>\n",
       "<code><a href=\"#load_mokoron\">load_mokoron</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>social</code>\n",
       "<code>sentiment</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "17&nbsp;633&nbsp;417\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1.86 Gb\n",
       "</td>\n",
       "<td>\n",
       "Russian Twitter sentiment markup\n",
       "</br>\n",
       "</br>\n",
       "Manually download https://www.dropbox.com/s/9egqjszeicki4ho/db.sql\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://dumps.wikimedia.org/\">Wikipedia</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_wiki\"></a>\n",
       "<code><a href=\"#load_wiki\">load_wiki</a></code>\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1&nbsp;541&nbsp;401\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "12.94 Gb\n",
       "</td>\n",
       "<td>\n",
       "Russian Wiki dump\n",
       "</br>\n",
       "</br>\n",
       "<code>wget https://dumps.wikimedia.org/ruwiki/latest/ruwiki-latest-pages-articles.xml.bz2</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://github.com/dialogue-evaluation/GramEval2020\">GramEval2020</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_gramru\"></a>\n",
       "<code><a href=\"#load_gramru\">load_gramru</a></code>\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "162&nbsp;372\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "30.04 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/dialogue-evaluation/GramEval2020/archive/master.zip</code>\n",
       "</br>\n",
       "<code>unzip master.zip</code>\n",
       "</br>\n",
       "<code>mv GramEval2020-master/dataTrain train</code>\n",
       "</br>\n",
       "<code>mv GramEval2020-master/dataOpenTest dev</code>\n",
       "</br>\n",
       "<code>rm -r master.zip GramEval2020-master</code>\n",
       "</br>\n",
       "<code>wget https://github.com/AlexeySorokin/GramEval2020/raw/master/data/GramEval_private_test.conllu</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"http://opencorpora.org/\">OpenCorpora</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_corpora\"></a>\n",
       "<code><a href=\"#load_corpora\">load_corpora</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>morph</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "4&nbsp;030\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "20.21 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.zip</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "RusVectores SimLex-965\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_simlex\"></a>\n",
       "<code><a href=\"#load_simlex\">load_simlex</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>emb</code>\n",
       "<code>sim</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://rusvectores.org/static/testsets/ru_simlex965_tagged.tsv</code>\n",
       "</br>\n",
       "<code>wget https://rusvectores.org/static/testsets/ru_simlex965.tsv</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://omnia-russica.github.io/\">Omnia Russica</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_omnia\"></a>\n",
       "<code><a href=\"#load_omnia\">load_omnia</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>morph</code>\n",
       "<code>web</code>\n",
       "<code>fiction</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "489.62 Gb\n",
       "</td>\n",
       "<td>\n",
       "Taiga + Wiki + Araneum. Read \"Even larger Russian corpus\" https://events.spbu.ru/eventsContent/events/2019/corpora/corp_sborn.pdf\n",
       "</br>\n",
       "</br>\n",
       "Manually download http://bit.ly/2ZT4BY9\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://github.com/dialogue-evaluation/factRuEval-2016/\">factRuEval-2016</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_factru\"></a>\n",
       "<code><a href=\"#load_factru\">load_factru</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>ner</code>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "254\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "969.27 Kb\n",
       "</td>\n",
       "<td>\n",
       "Manual PER, LOC, ORG markup prepared for 2016 Dialog competition\n",
       "</br>\n",
       "</br>\n",
       "<code>wget https://github.com/dialogue-evaluation/factRuEval-2016/archive/master.zip</code>\n",
       "</br>\n",
       "<code>unzip master.zip</code>\n",
       "</br>\n",
       "<code>rm master.zip</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://www.researchgate.net/publication/262203599_Introducing_Baselines_for_Russian_Named_Entity_Recognition\">Gareev</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_gareev\"></a>\n",
       "<code><a href=\"#load_gareev\">load_gareev</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>ner</code>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "97\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "455.02 Kb\n",
       "</td>\n",
       "<td>\n",
       "Manual PER, ORG markup (no LOC)\n",
       "</br>\n",
       "</br>\n",
       "Email Rinat Gareev (gareev-rm@yandex.ru) ask for dataset\n",
       "</br>\n",
       "<code>tar -xvf rus-ner-news-corpus.iob.tar.gz</code>\n",
       "</br>\n",
       "<code>rm rus-ner-news-corpus.iob.tar.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"http://www.labinform.ru/pub/named_entities/\">Collection5</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ne5\"></a>\n",
       "<code><a href=\"#load_ne5\">load_ne5</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>ner</code>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1&nbsp;000\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "2.96 Mb\n",
       "</td>\n",
       "<td>\n",
       "News articles with manual PER, LOC, ORG markup\n",
       "</br>\n",
       "</br>\n",
       "<code>wget http://www.labinform.ru/pub/named_entities/collection5.zip</code>\n",
       "</br>\n",
       "<code>unzip collection5.zip</code>\n",
       "</br>\n",
       "<code>rm collection5.zip</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://www.aclweb.org/anthology/I17-1042\">WiNER</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_wikiner\"></a>\n",
       "<code><a href=\"#load_wikiner\">load_wikiner</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>ner</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "203&nbsp;287\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "36.15 Mb\n",
       "</td>\n",
       "<td>\n",
       "Sentences from Wiki auto annotated with PER, LOC, ORG tags\n",
       "</br>\n",
       "</br>\n",
       "<code>wget https://github.com/dice-group/FOX/raw/master/input/Wikiner/aij-wikiner-ru-wp3.bz2</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"http://bsnlp.cs.helsinki.fi/shared_task.html\">BSNLP-2019</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_bsnlp\"></a>\n",
       "<code><a href=\"#load_bsnlp\">load_bsnlp</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>ner</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "464\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1.16 Mb\n",
       "</td>\n",
       "<td>\n",
       "Markup prepared for 2019 BSNLP Shared Task\n",
       "</br>\n",
       "</br>\n",
       "<code>wget http://bsnlp.cs.helsinki.fi/TRAININGDATA_BSNLP_2019_shared_task.zip</code>\n",
       "</br>\n",
       "<code>wget http://bsnlp.cs.helsinki.fi/TESTDATA_BSNLP_2019_shared_task.zip</code>\n",
       "</br>\n",
       "<code>unzip TRAININGDATA_BSNLP_2019_shared_task.zip</code>\n",
       "</br>\n",
       "<code>unzip TESTDATA_BSNLP_2019_shared_task.zip -d test_pl_cs_ru_bg</code>\n",
       "</br>\n",
       "<code>rm TRAININGDATA_BSNLP_2019_shared_task.zip TESTDATA_BSNLP_2019_shared_task.zip</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"http://ai-center.botik.ru/Airec/index.php/ru/collections/28-persons-1000\">Persons-1000</a>\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_persons\"></a>\n",
       "<code><a href=\"#load_persons\">load_persons</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>ner</code>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1&nbsp;000\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "2.96 Mb\n",
       "</td>\n",
       "<td>\n",
       "Same as Collection5, only PER markup + normalized names\n",
       "</br>\n",
       "</br>\n",
       "<code>wget http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://tatianashavrina.github.io/taiga_site/\">Taiga</a>\n",
       "</td>\n",
       "<td colspan=\"5\">\n",
       "Large collection of Russian texts from various sources: news sites, magazines, literacy, social networks\n",
       "</br>\n",
       "</br>\n",
       "<code>wget https://linghub.ru/static/Taiga/retagged_taiga.tar.gz</code>\n",
       "</br>\n",
       "<code>tar -xzvf retagged_taiga.tar.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Arzamas\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_arzamas\"></a>\n",
       "<code><a href=\"#load_taiga_arzamas\">load_taiga_arzamas</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "311\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "4.50 Mb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Fontanka\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_fontanka\"></a>\n",
       "<code><a href=\"#load_taiga_fontanka\">load_taiga_fontanka</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "342&nbsp;683\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "786.23 Mb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Interfax\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_interfax\"></a>\n",
       "<code><a href=\"#load_taiga_interfax\">load_taiga_interfax</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "46&nbsp;429\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "77.55 Mb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "KP\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_kp\"></a>\n",
       "<code><a href=\"#load_taiga_kp\">load_taiga_kp</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "45&nbsp;503\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "61.79 Mb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Lenta\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_lenta\"></a>\n",
       "<code><a href=\"#load_taiga_lenta\">load_taiga_lenta</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "36&nbsp;446\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "95.15 Mb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Taiga/N+1\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_nplus1\"></a>\n",
       "<code><a href=\"#load_taiga_nplus1\">load_taiga_nplus1</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "7&nbsp;696\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "24.96 Mb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Magazines\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_magazines\"></a>\n",
       "<code><a href=\"#load_taiga_magazines\">load_taiga_magazines</a></code>\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "39&nbsp;890\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "2.19 Gb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Subtitles\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_subtitles\"></a>\n",
       "<code><a href=\"#load_taiga_subtitles\">load_taiga_subtitles</a></code>\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "19&nbsp;011\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "909.08 Mb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Social\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_social\"></a>\n",
       "<code><a href=\"#load_taiga_social\">load_taiga_social</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>social</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1&nbsp;876&nbsp;442\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "648.18 Mb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Proza\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_proza\"></a>\n",
       "<code><a href=\"#load_taiga_proza\">load_taiga_proza</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>fiction</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1&nbsp;732&nbsp;434\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "38.25 Gb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Stihi\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_taiga_stihi\"></a>\n",
       "<code><a href=\"#load_taiga_stihi\">load_taiga_stihi</a></code>\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "9&nbsp;157&nbsp;686\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "12.80 Gb\n",
       "</td>\n",
       "<td>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://github.com/buriy/russian-nlp-datasets/releases\">Russian NLP Datasets</a>\n",
       "</td>\n",
       "<td colspan=\"5\">\n",
       "Several Russian news datasets from webhose.io, lenta.ru and other news sites.\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "News\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_buriy_news\"></a>\n",
       "<code><a href=\"#load_buriy_news\">load_buriy_news</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "2&nbsp;154&nbsp;801\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "6.84 Gb\n",
       "</td>\n",
       "<td>\n",
       "Dump of top 40 news + 20 fashion news sites.\n",
       "</br>\n",
       "</br>\n",
       "<code>wget https://github.com/buriy/russian-nlp-datasets/releases/download/r4/news-articles-2014.tar.bz2</code>\n",
       "</br>\n",
       "<code>wget https://github.com/buriy/russian-nlp-datasets/releases/download/r4/news-articles-2015-part1.tar.bz2</code>\n",
       "</br>\n",
       "<code>wget https://github.com/buriy/russian-nlp-datasets/releases/download/r4/news-articles-2015-part2.tar.bz2</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Webhose\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_buriy_webhose\"></a>\n",
       "<code><a href=\"#load_buriy_webhose\">load_buriy_webhose</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "285&nbsp;965\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "859.32 Mb\n",
       "</td>\n",
       "<td>\n",
       "Dump from webhose.io, 300 sources for one month.\n",
       "</br>\n",
       "</br>\n",
       "<code>wget https://github.com/buriy/russian-nlp-datasets/releases/download/r4/webhose-2016.tar.bz2</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://github.com/ods-ai-ml4sg/proj_news_viz/releases/tag/data\">ODS #proj_news_viz</a>\n",
       "</td>\n",
       "<td colspan=\"5\">\n",
       "Several news sites scraped by members of #proj_news_viz ODS project.\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Interfax\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ods_interfax\"></a>\n",
       "<code><a href=\"#load_ods_interfax\">load_ods_interfax</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "543&nbsp;961\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1.22 Gb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/ods-ai-ml4sg/proj_news_viz/releases/download/data/interfax.csv.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Gazeta\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ods_gazeta\"></a>\n",
       "<code><a href=\"#load_ods_gazeta\">load_ods_gazeta</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "865&nbsp;847\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1.63 Gb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/ods-ai-ml4sg/proj_news_viz/releases/download/data/gazeta.csv.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Izvestia\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ods_izvestia\"></a>\n",
       "<code><a href=\"#load_ods_izvestia\">load_ods_izvestia</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "86&nbsp;601\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "307.19 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/ods-ai-ml4sg/proj_news_viz/releases/download/data/iz.csv.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Meduza\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ods_meduza\"></a>\n",
       "<code><a href=\"#load_ods_meduza\">load_ods_meduza</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "71&nbsp;806\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "270.11 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/ods-ai-ml4sg/proj_news_viz/releases/download/data/meduza.csv.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "RIA\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ods_ria\"></a>\n",
       "<code><a href=\"#load_ods_ria\">load_ods_ria</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "101&nbsp;543\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "233.88 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/ods-ai-ml4sg/proj_news_viz/releases/download/data/ria.csv.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Russia Today\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ods_rt\"></a>\n",
       "<code><a href=\"#load_ods_rt\">load_ods_rt</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "106&nbsp;644\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "187.12 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/ods-ai-ml4sg/proj_news_viz/releases/download/data/rt.csv.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "TASS\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ods_tass\"></a>\n",
       "<code><a href=\"#load_ods_tass\">load_ods_tass</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>news</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1&nbsp;135&nbsp;635\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "3.27 Gb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/ods-ai-ml4sg/proj_news_viz/releases/download/data/tass-001.csv.gz</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://universaldependencies.org/\">Universal Dependencies</a>\n",
       "</td>\n",
       "<td colspan=\"5\">\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "GSD\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ud_gsd\"></a>\n",
       "<code><a href=\"#load_ud_gsd\">load_ud_gsd</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>morph</code>\n",
       "<code>syntax</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "5&nbsp;030\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1.01 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-GSD/raw/master/ru_gsd-ud-dev.conllu</code>\n",
       "</br>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-GSD/raw/master/ru_gsd-ud-test.conllu</code>\n",
       "</br>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-GSD/raw/master/ru_gsd-ud-train.conllu</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Taiga\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ud_taiga\"></a>\n",
       "<code><a href=\"#load_ud_taiga\">load_ud_taiga</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>morph</code>\n",
       "<code>syntax</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "3&nbsp;264\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "353.80 Kb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-Taiga/raw/master/ru_taiga-ud-dev.conllu</code>\n",
       "</br>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-Taiga/raw/master/ru_taiga-ud-test.conllu</code>\n",
       "</br>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-Taiga/raw/master/ru_taiga-ud-train.conllu</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "PUD\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ud_pud\"></a>\n",
       "<code><a href=\"#load_ud_pud\">load_ud_pud</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>morph</code>\n",
       "<code>syntax</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "1&nbsp;000\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "207.78 Kb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-PUD/raw/master/ru_pud-ud-test.conllu</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "SynTagRus\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_ud_syntag\"></a>\n",
       "<code><a href=\"#load_ud_syntag\">load_ud_syntag</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>morph</code>\n",
       "<code>syntax</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "61&nbsp;889\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "11.33 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-dev.conllu</code>\n",
       "</br>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-test.conllu</code>\n",
       "</br>\n",
       "<code>wget https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-train.conllu</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://github.com/dialogue-evaluation/morphoRuEval-2017\">morphoRuEval-2017</a>\n",
       "</td>\n",
       "<td colspan=\"5\">\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "General Internet-Corpus\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_morphoru_gicrya\"></a>\n",
       "<code><a href=\"#load_morphoru_gicrya\">load_morphoru_gicrya</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>morph</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "83&nbsp;148\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "10.58 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/dialogue-evaluation/morphoRuEval-2017/raw/master/GIKRYA_texts_new.zip</code>\n",
       "</br>\n",
       "<code>unzip GIKRYA_texts_new.zip</code>\n",
       "</br>\n",
       "<code>rm GIKRYA_texts_new.zip</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Russian National Corpus\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_morphoru_rnc\"></a>\n",
       "<code><a href=\"#load_morphoru_rnc\">load_morphoru_rnc</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>morph</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "98&nbsp;892\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "12.71 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/dialogue-evaluation/morphoRuEval-2017/raw/master/RNC_texts.rar</code>\n",
       "</br>\n",
       "<code>unrar x RNC_texts.rar</code>\n",
       "</br>\n",
       "<code>rm RNC_texts.rar</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "OpenCorpora\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_morphoru_corpora\"></a>\n",
       "<code><a href=\"#load_morphoru_corpora\">load_morphoru_corpora</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>morph</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "38&nbsp;510\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "4.80 Mb\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/dialogue-evaluation/morphoRuEval-2017/raw/master/OpenCorpora_Texts.rar</code>\n",
       "</br>\n",
       "<code>unrar x OpenCorpora_Texts.rar</code>\n",
       "</br>\n",
       "<code>rm OpenCorpora_Texts.rar</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://russe.nlpub.org/downloads/\">RUSSE Russian Semantic Relatedness</a>\n",
       "</td>\n",
       "<td colspan=\"5\">\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "HJ: Human Judgements of Word Pairs\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_russe_hj\"></a>\n",
       "<code><a href=\"#load_russe_hj\">load_russe_hj</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>emb</code>\n",
       "<code>sim</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/nlpub/russe-evaluation/raw/master/russe/evaluation/hj.csv</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "RT: Synonyms and Hypernyms from the Thesaurus RuThes\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_russe_rt\"></a>\n",
       "<code><a href=\"#load_russe_rt\">load_russe_rt</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>emb</code>\n",
       "<code>sim</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://raw.githubusercontent.com/nlpub/russe-evaluation/master/russe/evaluation/rt.csv</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "AE: Cognitive Associations from the Sociation.org Experiment\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_russe_ae\"></a>\n",
       "<code><a href=\"#load_russe_ae\">load_russe_ae</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>emb</code>\n",
       "<code>sim</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://github.com/nlpub/russe-evaluation/raw/master/russe/evaluation/ae-train.csv</code>\n",
       "</br>\n",
       "<code>wget https://github.com/nlpub/russe-evaluation/raw/master/russe/evaluation/ae-test.csv</code>\n",
       "</br>\n",
       "<code>wget https://raw.githubusercontent.com/nlpub/russe-evaluation/master/russe/evaluation/ae2.csv</code>\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"https://toloka.yandex.ru/datasets/\">Toloka Datasets</a>\n",
       "</td>\n",
       "<td colspan=\"5\">\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "Lexical Relations from the Wisdom of the Crowd (LRWC)\n",
       "</td>\n",
       "<td>\n",
       "<a name=\"load_toloka_lrwc\"></a>\n",
       "<code><a href=\"#load_toloka_lrwc\">load_toloka_lrwc</a></code>\n",
       "</td>\n",
       "<td>\n",
       "<code>emb</code>\n",
       "<code>sim</code>\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td align=\"right\">\n",
       "</td>\n",
       "<td>\n",
       "<code>wget https://tlk.s3.yandex.net/dataset/LRWC.zip</code>\n",
       "</br>\n",
       "<code>unzip LRWC.zip</code>\n",
       "</br>\n",
       "<code>rm LRWC.zip</code>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from corus.sources.meta import METAS\n",
    "from corus.readme import format_metas, show_html, patch_readme\n",
    "\n",
    "html = format_metas(METAS)\n",
    "show_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "floppy-beaver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-05 13:17:05--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.4, 198.51.44.8, 198.51.45.8, ...\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-releases.githubusercontent.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210405%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210405T101706Z&X-Amz-Expires=300&X-Amz-Signature=4b23292eb41af8807c5ce0e9ac4e34c5f92b57bb98565cac59ebc6260a8f6df6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=87156914&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-04-05 13:17:05--  https://github-releases.githubusercontent.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210405%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210405T101706Z&X-Amz-Expires=300&X-Amz-Signature=4b23292eb41af8807c5ce0e9ac4e34c5f92b57bb98565cac59ebc6260a8f6df6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=87156914&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.110.154, 185.199.111.154, 185.199.109.154, ...\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.110.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 527373240 (503M) [application/octet-stream]\n",
      "Saving to: ‘lenta-ru-news.csv.gz’\n",
      "\n",
      "lenta-ru-news.csv.g 100%[===================>] 502.94M  7.71MB/s    in 73s     \n",
      "\n",
      "2021-04-05 13:18:19 (6.85 MB/s) - ‘lenta-ru-news.csv.gz’ saved [527373240/527373240]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "realistic-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text, language='russian', delete_stop_words=False):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified string\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # 1. Перевести символы в нижний регистр\n",
    "    text = text.lower() #your code\n",
    "    \n",
    "    # 2.1 Заменить символы пунктуации на пробелы\n",
    "    text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2.2 Удалить \"плохие\" символы\n",
    "    text = re.sub('[^A-Za-z0-9]' if language == 'english' else '[^А-яа-я]', ' ', text)\n",
    "\n",
    "    \n",
    "    # 3. Применить WordNetLemmatizer\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    \n",
    "    # 4. Удалить стопслова.\n",
    "    if delete_stop_words:\n",
    "        stopWords = set(stopwords.words(language))\n",
    "        for stopWord in stopWords:\n",
    "            text = re.sub(r'\\b{}\\b'.format(stopWord), '', text)\n",
    "        \n",
    "    # 5. Удаляю пробелы у получая просто строку слов через пробел\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def get_grams_from_text(path='lenta-ru-news.csv.gz', \n",
    "                        n=2, \n",
    "                        amount_of_sentense=1000, \n",
    "                        verbose=True, \n",
    "                        show_how_much=1000, **kwargs):\n",
    "    records = load_lenta(path)\n",
    "    grams, count = {}, 1\n",
    "    flatten = lambda l: [' '.join(item) for sublist in l for item in sublist]\n",
    "    try:\n",
    "        while True and count != amount_of_sentense:\n",
    "            item = next(records).text\n",
    "            if verbose:\n",
    "                print(f'Sentence {count}') if count % show_how_much == 0 else 'pass'\n",
    "            \n",
    "            for i in np.arange(1, n+1):\n",
    "                if i not in list(grams.keys()):\n",
    "                    grams[i] = Counter()\n",
    "                ngram = [list(ngrams(text_prepare(sentense, **kwargs).lower().split(), n=i)) for sentense in nltk.sent_tokenize(item)]\n",
    "                grams[i] += Counter(flatten(ngram))\n",
    "            count +=1\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    finally:\n",
    "        del records\n",
    "    return grams\n",
    "\n",
    "\n",
    "def predict(corpus, sentence, n=3):\n",
    "    sen = text_prepare(sentence)\n",
    "    cor = corpus.copy()\n",
    "    rev = sen.split()[::-1]\n",
    "    s = sum(list(cor[2].values()))\n",
    "    s1 = sum(list(cor[1].values()))\n",
    "    d = {}\n",
    "    for key, value in list(cor[1].items()):\n",
    "        a = []\n",
    "        for i in np.arange(1, n+1):\n",
    "            v = cor[2][f'{rev[i-1]} {key}']\n",
    "            a.append(np.log(v / s) if v!=0 else np.log(0.000001))\n",
    "        d[key] = sum([np.log(value / s1)] + a)    \n",
    "    return sentence + ' ' + max(d.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "choice-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 2000\n",
      "Sentence 4000\n",
      "Sentence 6000\n"
     ]
    }
   ],
   "source": [
    "g = get_grams_from_text(n=1, \n",
    "                        amount_of_sentense=8000, \n",
    "                        show_how_much=2000, \n",
    "                        delete_stop_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attempted-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(g[1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-island",
   "metadata": {},
   "source": [
    "### 4.1.1. Алгоритм максимального соответствия (`MaximalMatching`)\n",
    "\n",
    "**Алгоритм максимального соответствия** -  это *жадный алгоритм*, в котором в тексте последовательно выделяются слова **наибольшей длины**, которые встретились в словаре. В качестве примера приведем строку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "difficult-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"вицепремьерноваядомашняярыба\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "educated-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_eng = \"themendinehere\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "centered-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_by(string, words, return_amount=False):\n",
    "    lowercaseCorpus = [x.lower() for x in words]\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    num_of_unknown_values = []\n",
    "    while i < len(string):\n",
    "        maxWord = \"\"\n",
    "        for j in range(i, len(string)):\n",
    "            tempWord = string[i:j+1]\n",
    "            if tempWord in lowercaseCorpus and len(tempWord) > len(maxWord):\n",
    "                maxWord = tempWord\n",
    "        if maxWord == '':\n",
    "            num_of_unknown_values.append(string[i])\n",
    "            maxWord = string[i]\n",
    "            \n",
    "        i = i+len(maxWord)\n",
    "        tokens.append(maxWord)\n",
    "    if return_amount:\n",
    "        return ' '.join(tokens), num_of_unknown_values\n",
    "    else:\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "injured-might",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вице премьер новая домашняя рыба'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximal_by(string, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "durable-travel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theme n dine here'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximal_by(s_eng, words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-private",
   "metadata": {},
   "source": [
    "### 4.1.2 Обратный алгоритм максимального соответствия\n",
    "\n",
    "В отличие от предыдущего алгоритма поиск оптимального разбиения начинается с конца строки. Таким образом, подпоследовательностью наибольшей длины окажется слово `here`.\n",
    "\n",
    "Тогда по данному методу получится строка:\n",
    "\n",
    "```python\n",
    "themendinehere - > the men dine here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "leading-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_by_reverse(string, words, return_amount=False):\n",
    "    lowercaseCorpus = [x.lower() for x in words]\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    string = string[::-1]\n",
    "    num_of_unknown_values = []\n",
    "    while i < len(string):\n",
    "        maxWord = \"\"\n",
    "        for j in range(i, len(string)):\n",
    "            tempWord = string[i:j+1][::-1]\n",
    "            if tempWord in lowercaseCorpus and len(tempWord) > len(maxWord):\n",
    "                maxWord = tempWord\n",
    "           \n",
    "        if maxWord == \"\":\n",
    "            num_of_unknown_values.append(string[i])\n",
    "            maxWord = string[i]\n",
    "        i = i+len(maxWord)\n",
    "        tokens.append(maxWord)\n",
    "    if return_amount:\n",
    "        return ' '.join(tokens[::-1]), num_of_unknown_values\n",
    "    else:\n",
    "        return ' '.join(tokens[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "twelve-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = words.words()\n",
    "k.remove('hemen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial-tulsa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the men dine here'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximal_by_reverse(s_eng, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tired-visitor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вице премьер новая домашняя рыба'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximal_by_reverse(string, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "exciting-tuner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'молоко кровавый ява разборки ва кровавый бангладеш ы ва ы ваи ванг ай вы ы в й молоко'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximal_by(\"молококровавыйяваразборкивакровавыйбангладешываываивангайвыывймолоко\", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hungarian-assistant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'молоко кровавый ява разбор кива кровавый бангладеш ы ва ы ва иван гай вы ы в й молоко'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximal_by_reverse(\"молококровавыйяваразборкивакровавыйбангладешываываивангайвыывймолоко\", l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-penny",
   "metadata": {},
   "source": [
    "### 4.1.3. Двунаправленный алгоритм максимального соответствия\n",
    "\n",
    "Двунаправленный алгоритм является комбинацией предыдущих алгоритмов. \n",
    "\n",
    "1. На первом шаге данного алгоритма выполняются прямой поиск, т.е. применяется алгоритм максимального соответствия и обратный поиск. \n",
    "\n",
    "2. Затем рассматриваются все полученные слова и выбирается наименее *сегментированная последовательность* (**с наименьшим количеством неизвестных слов**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "genuine-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_ways(string, words):\n",
    "    w1, s1 = maximal_by(string, words, return_amount=True)\n",
    "    w2, s2 = maximal_by_reverse(string, words, return_amount=True)\n",
    "    print(w1, s1)\n",
    "    print(w2, s2)\n",
    "    if s1 > s2:\n",
    "        return w2\n",
    "    else:\n",
    "        return w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eleven-luther",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вицепремьерноваядомашняярыба'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "massive-seeker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вице премьер новая домашняя рыба []\n",
      "вице премьер новая домашняя рыба []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'вице премьер новая домашняя рыба'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_ways(string, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "reliable-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ = list(filter(lambda x: len(x) != 1, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "agricultural-airline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "визу аль на я прогулка ['я']\n",
      "визу а льна я прогулка ['я', 'а']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'визу аль на я прогулка'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_ways(\"визуальнаяпрогулка\", new_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-divide",
   "metadata": {},
   "source": [
    "### 4.1.4 Выделение подпоследовательности с наименьшим количеством слов\n",
    "\n",
    "Указанный алгоритм рассматривает все возможные подпоследовательности и выбирает ту, где содержится наименьшее количество слов не из словаря. Ниже приведен пример реализации, где на каждом шаге из рассмотрения удаляются те ребра, по которым невозможна дальнейшая сегментация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "reported-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, value, in_vocabulary=True, is_terminal=False):\n",
    "        self.value = value\n",
    "        self.childrens = []\n",
    "        self.in_vocabulary = in_vocabulary\n",
    "        self.is_terminal = is_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "flush-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_by_rec(string, words, val=[], return_amount=False, use_words_not_in_vocab=False):\n",
    "    lowercaseCorpus = [x.lower() for x in words]\n",
    "    if val == []:\n",
    "        node = TreeNode(value='')\n",
    "    else:\n",
    "        node = val\n",
    "    i = 0\n",
    "    if node.value == '':\n",
    "        s = ''\n",
    "    else:\n",
    "        s = node.value + ' '\n",
    "    for j in range(i, len(string)):\n",
    "        tempWord = string[i:j+1]\n",
    "        if string[len(tempWord):] != '':\n",
    "            if tempWord in lowercaseCorpus:\n",
    "                node.childrens.append(TreeNode(value=s + tempWord))\n",
    "                maximal_by_rec(string[len(tempWord):], words, val=node.childrens[-1])\n",
    "            else:\n",
    "                node.childrens.append(TreeNode(value=s + tempWord, in_vocabulary=False))\n",
    "                if use_words_not_in_vocab:\n",
    "                    maximal_by_rec(string[len(tempWord):], words, val=node.childrens[-1])\n",
    "        else:\n",
    "            node.childrens.append(TreeNode(value=s + tempWord, is_terminal=True))\n",
    "     \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "temporal-comfort",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = maximal_by_rec(\"книгановейшаяигра\", new_, val=[], use_words_not_in_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cooperative-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def dfs(start, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    if start.is_terminal:\n",
    "        visited.add(start.value)\n",
    "    for next in set(start.childrens) - visited:\n",
    "        dfs(next, visited)\n",
    "    return visited\n",
    "\n",
    "def bfs(root): \n",
    "    visited, queue = set(), collections.deque([root])\n",
    "    visited.add(root)\n",
    "    m = []\n",
    "    while queue: \n",
    "        vertex = queue.popleft()\n",
    "        for neighbour in vertex.childrens: \n",
    "            visited.add(neighbour) \n",
    "            queue.append(neighbour)\n",
    "            if neighbour.is_terminal:\n",
    "                m.append(neighbour.value)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "experienced-snowboard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'кн иг ан овейшаяигра',\n",
       " 'кн иг ано вейшаяигра',\n",
       " 'кн иг ановейшаяигра',\n",
       " 'кн игановейшаяигра',\n",
       " 'книг ан овейшаяигра',\n",
       " 'книг ано вейшаяигра',\n",
       " 'книг ановейшаяигра',\n",
       " 'книга но вейшаяигра',\n",
       " 'книга новейшая иг ра',\n",
       " 'книга новейшая игр а',\n",
       " 'книга новейшая игра',\n",
       " 'книга новейшаяигра',\n",
       " 'книгановейшаяигра'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "capable-variable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['книгановейшаяигра',\n",
       " 'кн игановейшаяигра',\n",
       " 'книг ановейшаяигра',\n",
       " 'книга новейшаяигра',\n",
       " 'кн иг ановейшаяигра',\n",
       " 'книг ан овейшаяигра',\n",
       " 'книг ано вейшаяигра',\n",
       " 'книга но вейшаяигра',\n",
       " 'книга новейшая игра',\n",
       " 'кн иг ан овейшаяигра',\n",
       " 'кн иг ано вейшаяигра',\n",
       " 'книга новейшая иг ра',\n",
       " 'книга новейшая игр а']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfs(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
