{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "polished-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from download_text import get_grams_from_text\n",
    "from corus.sources.meta import METAS\n",
    "from corus.readme import format_metas, show_html, patch_readme\n",
    "from corus import load_lenta\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "peaceful-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pickle.load(open(\"../../../word_data/words.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "governing-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text, language):\n",
    "    shab = r'[a-z]+' if language == 'eng' else r'[а-я]+'\n",
    "    \"\"\"Возвращает список токенов (подряд идущих буквенных последовательностей) в тексте. \n",
    "       Текст при этом приводится к нижнему регистру.\"\"\"\n",
    "    return re.findall(shab, text.lower())\n",
    "\n",
    "def pdist(counter):\n",
    "    \"Превращает частоты из Counter в вероятностное распределение.\"\n",
    "    N = sum(list(counter.values()))\n",
    "    return lambda x: counter[x] / N\n",
    "\n",
    "def Pwords(words):\n",
    "    \"Вероятности слов, при условии, что они независимы.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Перемножим числа.  (Это как `sum`, только с умножением.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result\n",
    "\n",
    "P = pdist(COUNTS)\n",
    "\n",
    "WORDS = w[1]\n",
    "ALPHABET = {'ENG': 'abcdefghijklmnopqrstuvwxyz',\n",
    "            'RUS': 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'}\n",
    "COUNTS = WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-complaint",
   "metadata": {},
   "source": [
    "## 4.2 Вероятностные подходы - разбение слова на сегменты\n",
    "\n",
    "**Задача**: *Разбить полученную последовательность символов без пробелов на последовательность слов\n",
    "\n",
    "Вероятностные подходы основаны на оценке встречаемости каждого слова корпуса. В данном случае выбирается наиболее вероятная подпоследовательность слов, то есть:\n",
    "\n",
    "$$s^* = \\underset{s \\in S}{\\operatorname{s \\in S}} P(s) \\qquad (1)$$\n",
    "\n",
    "где $S$ - множество всех всевозможных подпоследовательностей слов из данной строки.\n",
    "\n",
    "### 4.2.1 Вероятностная языковая модель\n",
    "\n",
    "Наилучшая последовательность слов определяется формулой (1). Тогда при $n$ словах в последовательности $s$ вероятность разбиения:\n",
    "\n",
    "$$P(s) = \\prod\\limits_{k=1}^n P(w_k)$$\n",
    "\n",
    "где $P(w_k)$ - вероятность встретить в тексте слово $w_k$.\n",
    "\n",
    "Рассмотрим строку `the men dine here`. Для оценки вероятностей воспользуемся открытым [словарем](http://norvig.com/ngrams/). Данный словарь содержит более триллиона слов. В используемом корпусе приведенные слова встретились:\n",
    "\n",
    "- `the` - 23135851162 раза;\n",
    "- `men` - 174058407 раз;\n",
    "- `dine` - 2012279 раз;\n",
    "- `here` – 639711198 раз\n",
    "\n",
    "Всего слов в корпусе: `1024908267229`. Тогда:\n",
    "\n",
    "$$P(\\operatorname{the men dine here}) = P(\\operatorname{the}) P(\\operatorname{men}) P(\\operatorname{die}) P(\\operatorname{here}) = 4.698 × 10-1$$\n",
    "\n",
    "Так как $P(w) \\in [0,1]$, то при перемножении вероятностей возникает проблема арифметического переполнения снизу. Чтобы этого избежать часто прибегают к логарифмированию:\n",
    "\n",
    "$$\\log P(s) = \\log \\prod\\limits_{k=1}^n P(w_k) = \\sum\\limits_{k=1}^n \\log P(w_k)$$\n",
    "\n",
    "Другой возможной проблемой является **отсутствие встретившегося слова** в словаре. Оценка вероятности встретить данное слово равно нулю. Чтобы не возникало такой ситуации, применяют **аддитивное сглаживание (сглаживание Лапласа)**, которое состоит в\n",
    "искусственном *добавлении единицы к встречаемости каждого слова*:\n",
    "\n",
    "$$P(w_i) = \\frac{v_i + 1}{\\sum\\limits_{i \\in V} (v_i+1)} = \\frac{v_i+1}{|V| + \\sum\\limits_{i \\in V}v_i}$$\n",
    "\n",
    "где $v_i$ - сколько раз встретилось слово $w_i$, $V$ - словарь.\n",
    "\n",
    "Соответственно, **1-ый подход** заключается в следующем: перенумеруем все возможные разбиения и выберем то, у которого максимальная `Pwords`\n",
    "\n",
    "Как выбрать количество сегментов для строки длины *n* - этим мы занимались в прошлых пунтах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-somewhere",
   "metadata": {},
   "source": [
    "### 4.2.2. `First Word + Other Words`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-composite",
   "metadata": {},
   "source": [
    "**Подход 2:** Делаем одно разбиение - на первое слово и все остальное.  Если предположить, что слова независимы,\n",
    "можно максимизировать вероятность первого слова + лучшего разбиения оставшихся букв.\n",
    "    \n",
    "    assert segment('choosespain') == ['choose', 'spain']\n",
    "\n",
    "    segment('choosespain') ==\n",
    "       max(Pwords(['c'] + segment('hoosespain')),\n",
    "           Pwords(['ch'] + segment('oosespain')),\n",
    "           Pwords(['cho'] + segment('osespain')),\n",
    "           Pwords(['choo'] + segment('sespain')),\n",
    "           ...\n",
    "           Pwords(['choosespain'] + segment('')))\n",
    "       \n",
    "    \n",
    "       \n",
    "Чтобы сделать это хоть сколько-нибудь эффективным, нужно избежать слишком большого числа пересчетов оставшейся части слова.  Это можно сделать или с помощью [динамического программирования](https://habr.com/ru/post/113108/) или с помощью [мемоизации](https://ru.wikipedia.org/wiki/Мемоизация) кэширования. \n",
    "\n",
    "Кроме того, для первого слова не обязательно брать все возможные варианты разбиений - мы можем установить максимальную длину - чуть большую, чем длина самого длинного слова, которое мы видели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "filled-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Запомнить результаты исполнения функции f, чьи аргументы args должны быть хешируемыми.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-container",
   "metadata": {},
   "source": [
    "- [мемоизация в python](https://habr.com/ru/post/335866/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "equipped-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = {len(w): w for w in COUNTS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "assured-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=29):\n",
    "    \"Вернуть список всех пар (a, b); start <= len(a) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "boolean-weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'слово'), ('с', 'лово'), ('сл', 'ово'), ('сло', 'во'), ('слов', 'о'), ('слово', '')]\n",
      "[('о', 'ченьдлинноеслово'), ('оч', 'еньдлинноеслово'), ('оче', 'ньдлинноеслово'), ('очен', 'ьдлинноеслово'), ('очень', 'длинноеслово'), ('оченьд', 'линноеслово'), ('оченьдл', 'инноеслово'), ('оченьдли', 'нноеслово'), ('оченьдлин', 'ноеслово'), ('оченьдлинн', 'оеслово')]\n"
     ]
    }
   ],
   "source": [
    "print(splits('слово'))\n",
    "print(splits('оченьдлинноеслово', 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "integral-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Вернуть список слов, который является наиболее вероятной сегментацией нашего текста.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "otherwise-march",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['новая', 'страна']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('новаястрана')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "blocked-campus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['бонусная', 'игра']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('бонуснаяигра')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "partial-bidder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тыполюбитьзаставиласебячтобыплеснутьмневдушучернымядом\n",
      "['ты', 'полюбить', 'заставила', 'себя', 'чтобы', 'плеснуть', 'мне', 'в', 'душу', 'черным', 'ядом']\n",
      "2.0531571169041637e-48\n",
      "4.215454146694218e-96\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(new[54])\n",
    "print(segment(new[54]))\n",
    "print(Pwords(segment(new[54])))\n",
    "print(Pwords(segment(new[54] * 2)))\n",
    "print(Pwords(segment(new[54] * 7))) #проблема переполнения разрядности чисел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "developmental-account",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чемпионатфотографийплохойсобаки\n",
      "['чемпионат', 'фотографий', 'плохой', 'собаки']\n"
     ]
    }
   ],
   "source": [
    "print(new[31])\n",
    "print(segment(new[31]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-compensation",
   "metadata": {},
   "source": [
    "- выглядит неплохо\n",
    "- предположение о мешке слов имеет ряд ограничений.\n",
    "- пересчет Pwords на каждом вызове выглядит неэффективным.\n",
    "- переполнение чисел возникает для текстов длинее +- 100 слов; придется использовать логарифмическое сглаживание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-billy",
   "metadata": {},
   "source": [
    "\n",
    "## Динамическое программирование. Насколько дорого превращать одно слово в другое?\n",
    "\n",
    "<a name='4-1'></a>\n",
    "\n",
    "Динамическое программирование позволяет разбить задачу на подзадачи, решив которые можно скомпоновать финальное решение. Будем пытаться превратить строку *source* `[0..i]` в строку *target* `[0..j]`, сосчитаем все возможные комбинации подстрок `substrings[i, j]` и рассчитаем их `edit_distance` до исходной строки. Будем сохранять результаты в таблицу и переиспользовать их для расчета дальнейших изменений.\n",
    "\n",
    "Необходимо создать матрицу такого вида:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-shore",
   "metadata": {},
   "source": [
    "$$\\text{Initial}$$\n",
    "\n",
    "\\begin{align}\n",
    "D[0,0] &= 0 \\\\\n",
    "D[i,0] &= D[i-1,0] + del\\_cost(source[i]) \\tag{4}\\\\\n",
    "D[0,j] &= D[0,j-1] + ins\\_cost(target[j]) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-proportion",
   "metadata": {},
   "source": [
    "$$\\text{Operation}$$\n",
    "\\begin{align}\n",
    " \\\\\n",
    "D[i,j] =min\n",
    "\\begin{cases}\n",
    "D[i-1,j] + del\\_cost\\\\\n",
    "D[i,j-1] + ins\\_cost\\\\\n",
    "D[i-1,j-1] + \\left\\{\\begin{matrix}\n",
    "rep\\_cost; & if src[i]\\neq tar[j]\\\\\n",
    "0 ; & if src[i]=tar[j]\n",
    "\\end{matrix}\\right.\n",
    "\\end{cases}\n",
    "\\tag{5}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-sunset",
   "metadata": {},
   "source": [
    "Таким образом, превратить слово **play** в слово **stay** при стоимости вставки 1, стоимости удаления 1, и стоимости замены 2 даст такую таблицу:\n",
    "<table style=\"width:20%\">\n",
    "\n",
    "  <tr>\n",
    "    <td> <b> </b>  </td>\n",
    "    <td> <b># </b>  </td>\n",
    "    <td> <b>s </b>  </td>\n",
    "    <td> <b>t </b> </td> \n",
    "    <td> <b>a </b> </td> \n",
    "    <td> <b>y </b> </td> \n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td> <b>  #  </b></td>\n",
    "    <td> 0</td> \n",
    "    <td> 1</td> \n",
    "    <td> 2</td> \n",
    "    <td> 3</td> \n",
    "    <td> 4</td> \n",
    " \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>  p  </b></td>\n",
    "    <td> 1</td> \n",
    " <td> 2</td> \n",
    "    <td> 3</td> \n",
    "    <td> 4</td> \n",
    "   <td> 5</td>\n",
    "  </tr>\n",
    "   \n",
    "  <tr>\n",
    "    <td> <b> l </b></td>\n",
    "    <td>2</td> \n",
    "    <td>3</td> \n",
    "    <td>4</td> \n",
    "    <td>5</td> \n",
    "    <td>6</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> <b> a </b></td>\n",
    "    <td>3</td> \n",
    "     <td>4</td> \n",
    "     <td>5</td> \n",
    "     <td>4</td>\n",
    "     <td>5</td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td> <b> y </b></td>\n",
    "    <td>4</td> \n",
    "      <td>5</td> \n",
    "     <td>6</td> \n",
    "     <td>5</td>\n",
    "     <td>4</td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "recognized-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost=1, del_cost=1, rep_cost=2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: строка-исходник\n",
    "        target: строка, в которую мы должны исходник превратить\n",
    "        ins_cost: цена вставки\n",
    "        del_cost: цена удаления\n",
    "        rep_cost: цена замены буквы\n",
    "    Output:\n",
    "        D: матрица размера len(source)+1 на len(target)+1 содержащая минимальные расстояния edit_distance\n",
    "        med: минимальное расстояние edit_distance (med), необходимое, \n",
    "        чтобы превратить строку source в строку target\n",
    "    '''\n",
    "    # стоимость удаления и вставки = 1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "\n",
    "    # Заткнем нашу матрицу нулями\n",
    "    D = np.zeros((m + 1, \n",
    "                  n + 1), dtype=int) \n",
    "    \n",
    "    # Заполним первую колонку\n",
    "    for row in range(1, m + 1): \n",
    "        D[row, 0] = D[row - 1, 0] + del_cost\n",
    "        \n",
    "    # Заполним первую строку\n",
    "    for col in range(1, n+1): \n",
    "        D[0, col] = D[0, col - 1] + ins_cost\n",
    "        \n",
    "    # Теперь пойдем от 1 к m-той строке\n",
    "    for row in range(1, m + 1): \n",
    "        \n",
    "        # итерируемся по колонкам от 1 до n\n",
    "        for col in range(1, n + 1):\n",
    "            \n",
    "            # r_cost - стоимость замены\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Совпадает ли буква исходного слова из предыдущей строки\n",
    "            # с буквой целевого слова из предыдущей колонки, \n",
    "            if source[row - 1] == target[col - 1]:\n",
    "                # Если они не нужны, то замена не нужна -> стоимость = 0\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Обновляем значение ячейки на базе предыдущих значений \n",
    "            # Считаем D[i,j] как минимум из трех возможных стоимостей (как в формуле выше)\n",
    "            D[row, col] = min([D[row - 1, col] + del_cost, \n",
    "                               D[row, col - 1] + ins_cost, \n",
    "                               D[row - 1, col - 1] + r_cost])\n",
    "          \n",
    "    # установить edit_distance в значение из правого нижнего угла\n",
    "    med = D[m,n]\n",
    "    \n",
    "\n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "general-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расстояние:  6 \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n",
      "d  5  6  7  6  5\n",
      "s  6  5  6  7  6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "source =  'playds'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "\n",
    "print(\"Расстояние: \",min_edits, \"\\n\")\n",
    "\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-stanley",
   "metadata": {},
   "source": [
    "### 4.2.2 Биграммы\n",
    "\n",
    "Предудыщий метод рассматривает каждое слово в отдельности, однако не учитывает соседние слова, которые также могут помочь правильно разбить текст. Чтобы учесть сочетания слов рассматриваются **биграммы** – сочетания по два слова и на основе вероятностей встречаемости *сочетаний слов* восстановить пробелы.\n",
    "\n",
    "Строка `the men dine here` с учётом начала и конца предложения может быть разбита на $5$ биграмм:\n",
    "\n",
    "1. begin the\n",
    "2. the men\n",
    "3. men dine\n",
    "4. dine here\n",
    "5. here end\n",
    "\n",
    "Формула (2) тогда принимает вид:\n",
    "\n",
    "$$P(s) = \\prod\\limits_{k=1}^n P(w_k | w_{k-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-ordinance",
   "metadata": {},
   "source": [
    "Чуть менее неправильная аппроксимация:\n",
    "    \n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_2) \\ldots  \\times \\ldots P(w_n \\mid w_{n-1})$\n",
    "\n",
    "Эта штука называется *биграммной* моделью. Представьте, что вы взяли текст, достали из него все возможные пары подряд идущих слов и положили каждую пару в мешок, промаркированный ПЕРВЫМ словом из пары.  После этого, чтобы сгенерировать кусок текста, мы берем первое слово из исходного мешка слов , а каждое следующее слово вынимаем из соответствующего мешка биграмм. \n",
    "\n",
    "Начнем с определения вероятности текущего слова при условии данного предыдущего слова из Counter:\n",
    "\n",
    "Отмечу, что для английского языка биграммная модель будет выглядеть так:\n",
    "    \n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_2) \\ldots  \\times \\ldots P(w_n \\mid w_{n-1})$\n",
    "\n",
    "условная вероятность слова при условии предыдущего слова определяется так:\n",
    "\n",
    "$P(w_n \\mid w_{n-1}) = P(w_{n-1}w_n) / P(w_{n-1}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acting-greensboro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 5000\n",
      "Sentence 10000\n"
     ]
    }
   ],
   "source": [
    "bigrams = get_grams_from_text(path='../../../word_data/lenta-ru-news.csv.gz',\n",
    "                              n=[2], \n",
    "                              amount_of_sentense=15000, \n",
    "                              show_how_much=5000, \n",
    "                              delete_stop_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "crude-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS1 = w[1]\n",
    "COUNTS2 = bigrams[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bibliographic-radiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337838 16167199\n",
      "1062637 2401223\n"
     ]
    }
   ],
   "source": [
    "print(len(COUNTS1), sum(list(COUNTS1.values())))\n",
    "print(len(COUNTS2), sum(list(COUNTS2.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "auburn-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "P1w = pdist(COUNTS1)\n",
    "P2w = pdist(COUNTS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "wireless-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords2(words, prev='<S>'):\n",
    "    \"Вероятность последовательности слов с помощью биграммной модели(при условии предыдущего слова).\"\n",
    "    return product(cPword(w, (prev if (i == 0) else words[i-1]) )\n",
    "                   for (i, w) in enumerate(words))\n",
    "\n",
    "# Перепишем Pwords на большой словарь P1w вместо Pword\n",
    "def Pwords(words):\n",
    "    \"Вероятности слов при условии их независимости.\"\n",
    "    return product(P1w(w) for w in words)\n",
    "\n",
    "def cPword(word, prev):\n",
    "    \"Условная вероятность слова при условии предыдущего.\"\n",
    "    bigram = prev + ' ' + word\n",
    "    if P2w(bigram) > 0 and P1w(prev) > 0:\n",
    "        return P2w(bigram) / P1w(prev)\n",
    "    else: # если что-то не встретилось, поставим среднее между P1w и 0\n",
    "        return P1w(word) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "unavailable-success",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4317520743662085e-16\n",
      "2.2642865047148052e-14\n",
      "2.2642865047148052e-14\n",
      "4.0198450464788803e-17\n"
     ]
    }
   ],
   "source": [
    "print(Pwords(tokens('я знаю эту книгу', language='rus')))\n",
    "print(Pwords2(tokens('я знаю эту книгу', language='rus')))\n",
    "print(Pwords2(tokens('эту книгу я знаю', language='rus')))\n",
    "print(Pwords2(tokens('эту книгу знаю я', language='rus')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-pierre",
   "metadata": {},
   "source": [
    "Чтобы сделать `segment2`, скопируем `segment`, добавим в аргументы предыдущий токен, а вероятности будем считать с помощью `Pwords2` вместо `Pwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "forbidden-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo \n",
    "def segment2(text, prev='<S>'): \n",
    "    \"Возвращает наилучшее разбиение текста, используя статистику биграмм.\" \n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment2(rest, first) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=lambda words: Pwords2(words, prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "regional-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['новая', 'страна']\n",
      "['большая', 'и', 'маленькая', 'страна']\n",
      "['произведение', 'искусства', 'невообразимой', 'красоты']\n",
      "['ты', 'полюбить', 'заставила', 'себя', 'чтобы', 'плеснуть', 'мне', 'в', 'душу', 'черным', 'ядом']\n"
     ]
    }
   ],
   "source": [
    "print(segment2('новаястрана'))\n",
    "print(segment2('большаяималенькаястрана'))\n",
    "print(segment2('произведениеискусстваневообразимойкрасоты'))\n",
    "print(segment2(new[54]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "adequate-accessory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'тамгденаснетгоритневиданныйрассветгденаснетмореирубиновыйзакатгденаснетлескакмалахитовыйбраслетгденаснетналебединыхостровахгденаснетуслышьменяивытащиизомутаведивмойвымышленныйгородвымощенныйзолотомвоснеявижудалииноземныегдемилосердиеправитгдеберегакисельные'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxxy = \"\"\"Там, где нас нет, горит невиданный рассвет.\n",
    "Где нас нет - море и рубиновый закат.\n",
    "Где нас нет - лес, как малахитовый браслет.\n",
    "Где нас нет, на Лебединых островах.\n",
    "Где нас нет, услышь меня и вытащи из омута.\n",
    "Веди в мой вымышленный город, вымощенный золотом.\n",
    "Во сне я вижу дали иноземные.\n",
    "Где милосердие правит, где берега кисельные.\"\"\"\n",
    "oxxy = ''.join(re.findall(r'[а-я]',oxxy.lower()))\n",
    "oxxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "incorporated-effectiveness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['там', 'где', 'нас', 'нет', 'горит', 'не', 'в', 'и', 'данный', 'рассвет', 'где', 'нас', 'нет', 'море', 'и', 'рубиновый', 'закат', 'где', 'нас', 'нет', 'лес', 'как', 'малахит', 'о', 'вый', 'браслет', 'где', 'нас', 'нет', 'на', 'лебеди', 'ных', 'островах', 'где', 'нас', 'нет', 'услышь', 'меня', 'и', 'вы', 'та', 'щи', 'из', 'омут', 'а', 'веди', 'в', 'мой', 'вымышленный', 'город', 'вы', 'мощен', 'ный', 'золотом', 'во', 'с', 'не', 'я', 'вижу', 'дали', 'иноземные', 'где', 'милосердие', 'правит', 'где', 'берега', 'кисельные']\n",
      "['там', 'где', 'нас', 'нет', 'горит', 'не', 'в', 'и', 'данный', 'рассвет', 'где', 'нас', 'нет', 'море', 'и', 'рубиновый', 'закат', 'где', 'нас', 'нет', 'лес', 'как', 'малахит', 'о', 'вый', 'браслет', 'где', 'нас', 'нет', 'на', 'лебеди', 'ных', 'островах', 'где', 'нас', 'нет', 'услышь', 'меня', 'и', 'вы', 'та', 'щи', 'из', 'омут', 'а', 'веди', 'в', 'мой', 'вымышленный', 'город', 'вы', 'мощен', 'ный', 'золотом', 'во', 'сне', 'я', 'вижу', 'дали', 'иноземные', 'где', 'милосердие', 'правит', 'где', 'берега', 'кисельные']\n"
     ]
    }
   ],
   "source": [
    "print(segment(oxxy))\n",
    "print(segment2(oxxy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "unsigned-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bigrams, open(\"../../../word_data/bigrams.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "demographic-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = pickle.load(open(\"../../../word_data/bigrams.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-craps",
   "metadata": {},
   "source": [
    "## 4.2.3 Осталась валидация - насколько модели качественные"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma_paper",
   "language": "python",
   "name": "diploma_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
