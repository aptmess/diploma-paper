{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oriented-provider",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aptmess/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aptmess/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/aptmess/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "# Corus - NLP datasets\n",
    "import corus\n",
    "from corus import load_lenta\n",
    "\n",
    "#NLTK - Natural Language Tool Kit\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk import ngrams\n",
    "\n",
    "#Other\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from tqdm import notebook\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical as to_ct\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "subject-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    \"\"\"Tokenize the corpus text.\n",
    "    :param corpus: list containing a string of text (example: [\"I like playing football with my friends\"])\n",
    "    :return corpus_tokenized: indexed list of words in the corpus, in the same order as the original corpus (the example above would return [[1, 2, 3, 4]])\n",
    "    :return V: size of vocabulary\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus_tokenized = tokenizer.texts_to_sequences(corpus)\n",
    "    V = len(tokenizer.word_index)\n",
    "    return corpus_tokenized, V\n",
    "\n",
    "def to_categorical(y, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "    # Returns\n",
    "        A binary matrix representation of the input.\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "\n",
    "def corpus2io(corpus_tokenized, V, window_size):\n",
    "    \"\"\"Converts corpus text into context and center words\n",
    "    # Arguments\n",
    "        corpus_tokenized: corpus text\n",
    "        window_size: size of context window\n",
    "    # Returns\n",
    "        context and center words (arrays)\n",
    "    \"\"\"\n",
    "    for words in corpus_tokenized:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels = []\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            contexts.append([words[i]-1 for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word-1)\n",
    "            x = np_utils.to_categorical(contexts, V)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y.ravel())\n",
    "\n",
    "            \n",
    "def softmax(x):\n",
    "    \"\"\"Calculate softmax based probability for given input vector\n",
    "    # Arguments\n",
    "        x: numpy array/list\n",
    "    # Returns\n",
    "        softmax of input array\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def cbow(context, label, W1, W2, loss, lr):\n",
    "    \"\"\"\n",
    "    Implementation of Continuous-Bag-of-Words Word2Vec model\n",
    "    :param context: all the context words (these represent the inputs)\n",
    "    :param label: the center word (this represents the label)\n",
    "    :param W1: weights from the input to the hidden layer\n",
    "    :param W2: weights from the hidden to the output layer\n",
    "    :param loss: float that represents the current value of the loss function\n",
    "    :return: updated weights and loss\n",
    "    \"\"\"\n",
    "    x = np.mean(context, axis=(0, 1))\n",
    "    h = np.dot(W1.T, x)\n",
    "    u = np.dot(W2.T, h)\n",
    "    y_pred = softmax(u)\n",
    "\n",
    "    e = -label + y_pred\n",
    "    dW2 = np.outer(h, e)\n",
    "    dW1 = np.outer(x, np.dot(W2, e))\n",
    "\n",
    "    new_W1 = W1 - lr * dW1\n",
    "    new_W2 = W2 - lr * dW2\n",
    "\n",
    "    loss += -float(u[label == 1]) + np.log(np.sum(np.exp(u)))\n",
    "\n",
    "    return new_W1, new_W2, loss\n",
    "\n",
    "\n",
    "def text_prepare(text, language='russian', delete_stop_words=False):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified string\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # 1. Перевести символы в нижний регистр\n",
    "    text = text.lower() #your code\n",
    "    \n",
    "    # 2.1 Заменить символы пунктуации на пробелы\n",
    "    text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2.2 Удалить \"плохие\" символы\n",
    "    text = re.sub('[^A-Za-z0-9]' if language == 'english' else '[^А-яа-я]', ' ', text)\n",
    "\n",
    "    \n",
    "    # 3. Применить WordNetLemmatizer\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    \n",
    "    # 4. Удалить стопслова.\n",
    "    if delete_stop_words:\n",
    "        stopWords = set(stopwords.words(language))\n",
    "        for stopWord in stopWords:\n",
    "            text = re.sub(r'\\b{}\\b'.format(stopWord), '', text)\n",
    "        \n",
    "    # 5. Удаляю пробелы у получая просто строку слов через пробел\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text(path='../../../data/lenta-ru-news.csv.gz', \n",
    "                        amount_of_sentense=1000, \n",
    "                        verbose=True, \n",
    "                        show_how_much=1000, **kwargs):\n",
    "    records = load_lenta(path)\n",
    "    a = []\n",
    "    count = 1\n",
    "    try:\n",
    "        while True and count != amount_of_sentense:\n",
    "            item = next(records).text\n",
    "            if verbose:\n",
    "                print(f'Sentence {count}') if count % show_how_much == 0 else 'pass'\n",
    "            a.append(text_prepare(item))\n",
    "            count +=1\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    finally:\n",
    "        del records\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "neutral-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec():\n",
    "    \n",
    "    def __init__(self, d=50, h=5):\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        \n",
    "    def fit(self, coprusI, num_epochs=1, lr=0.1):\n",
    "        np.random.seed(100)\n",
    "        print('Start counting dictionary')\n",
    "        self.corpus_tokenized, self.V = tokenize(coprusI)\n",
    "        print(f'Vocabulary size {self.V}')\n",
    "        my = zip(self.corpus_tokenized, coprusI)\n",
    "        print('All Words')\n",
    "        self.vocabulary = {}\n",
    "        self.r = {}\n",
    "        for i, j in my:\n",
    "            u = j.split()\n",
    "            for m, n in zip(i, u):\n",
    "                self.vocabulary[n] = m-1\n",
    "                self.r[m-1] = n\n",
    "        print('Start fitting')\n",
    "        E = np.random.rand(self.V, self.d)\n",
    "        C = np.random.rand(self.d, self.V)\n",
    "        print(f'Emb: {E.shape}, Context: {C.shape}, {len(list(self.vocabulary.keys()))}')\n",
    "        loss = 0.\n",
    "        for num in range(num_epochs):\n",
    "            print(f'epoch {num}')\n",
    "            for i, (context, label) in enumerate(corpus2io(self.corpus_tokenized, self.V, self.h)):\n",
    "                E, C, loss = cbow(context, label, E, C, loss, lr)\n",
    "                if i % 1000 ==0 :\n",
    "                    print(f\"\\n\\t loss = {loss}\\n\")\n",
    "                    print(f'Word {i}')\n",
    "        self.embedding = E\n",
    "        self.context = C\n",
    "        \n",
    "    def predict(self, x):\n",
    "        prob = softmax(np.dot(self.context.T, np.dot(self.embedding.T, to_ct(self.vocabulary[x], num_classes=self.V))))\n",
    "        return self.r[np.argmax(prob)]\n",
    "    \n",
    "    def emb(self, word):\n",
    "        if word in self.vocabulary:\n",
    "            \n",
    "            return self.embedding[self.vocabulary[word]-1]\n",
    "        else:\n",
    "            return f'No {word}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mature-testing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1000\n",
      "Sentence 2000\n",
      "Sentence 3000\n",
      "Sentence 4000\n",
      "Sentence 5000\n",
      "Sentence 6000\n",
      "Sentence 7000\n",
      "Sentence 8000\n",
      "Sentence 9000\n",
      "Sentence 10000\n",
      "Sentence 11000\n",
      "Sentence 12000\n",
      "Sentence 13000\n",
      "Sentence 14000\n",
      "Sentence 15000\n",
      "Sentence 16000\n",
      "Sentence 17000\n",
      "Sentence 18000\n",
      "Sentence 19000\n",
      "Sentence 20000\n",
      "Sentence 21000\n",
      "Sentence 22000\n",
      "Sentence 23000\n",
      "Sentence 24000\n",
      "Sentence 25000\n",
      "Sentence 26000\n",
      "Sentence 27000\n",
      "Sentence 28000\n",
      "Sentence 29000\n",
      "Sentence 30000\n",
      "Sentence 31000\n",
      "Sentence 32000\n",
      "Sentence 33000\n",
      "Sentence 34000\n",
      "Sentence 35000\n",
      "Sentence 36000\n",
      "Sentence 37000\n",
      "Sentence 38000\n",
      "Sentence 39000\n",
      "Sentence 40000\n",
      "Sentence 41000\n",
      "Sentence 42000\n",
      "Sentence 43000\n",
      "Sentence 44000\n",
      "Sentence 45000\n",
      "Sentence 46000\n",
      "Sentence 47000\n",
      "Sentence 48000\n",
      "Sentence 49000\n",
      "Sentence 50000\n",
      "Sentence 51000\n",
      "Sentence 52000\n",
      "Sentence 53000\n",
      "Sentence 54000\n",
      "Sentence 55000\n",
      "Sentence 56000\n",
      "Sentence 57000\n",
      "Sentence 58000\n",
      "Sentence 59000\n",
      "Sentence 60000\n",
      "Sentence 61000\n",
      "Sentence 62000\n",
      "Sentence 63000\n",
      "Sentence 64000\n",
      "Sentence 65000\n",
      "Sentence 66000\n",
      "Sentence 67000\n",
      "Sentence 68000\n",
      "Sentence 69000\n",
      "Sentence 70000\n",
      "Sentence 71000\n",
      "Sentence 72000\n",
      "Sentence 73000\n",
      "Sentence 74000\n",
      "Sentence 75000\n",
      "Sentence 76000\n",
      "Sentence 77000\n",
      "Sentence 78000\n",
      "Sentence 79000\n",
      "Sentence 80000\n",
      "Sentence 81000\n",
      "Sentence 82000\n",
      "Sentence 83000\n",
      "Sentence 84000\n",
      "Sentence 85000\n",
      "Sentence 86000\n",
      "Sentence 87000\n",
      "Sentence 88000\n",
      "Sentence 89000\n",
      "Sentence 90000\n",
      "Sentence 91000\n",
      "Sentence 92000\n",
      "Sentence 93000\n",
      "Sentence 94000\n",
      "Sentence 95000\n",
      "Sentence 96000\n",
      "Sentence 97000\n",
      "Sentence 98000\n",
      "Sentence 99000\n"
     ]
    }
   ],
   "source": [
    "corpus = get_text(amount_of_sentense=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accomplished-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = get_text(amount_of_sentense=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-rhythm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start counting dictionary\n",
      "Vocabulary size 337838\n",
      "All Words\n",
      "Start fitting\n",
      "Emb: (337838, 50), Context: (50, 337838), 337838\n",
      "epoch 0\n",
      "\n",
      "\t loss = 13.344753096447512\n",
      "\n",
      "Word 0\n",
      "\n",
      "\t loss = 11906.455836463838\n",
      "\n",
      "Word 1000\n",
      "\n",
      "\t loss = 23151.27835878937\n",
      "\n",
      "Word 2000\n",
      "\n",
      "\t loss = 33970.182995615745\n",
      "\n",
      "Word 3000\n",
      "\n",
      "\t loss = 44517.539929755956\n",
      "\n",
      "Word 4000\n",
      "\n",
      "\t loss = 54921.5257844792\n",
      "\n",
      "Word 5000\n",
      "\n",
      "\t loss = 64900.16113174904\n",
      "\n",
      "Word 6000\n",
      "\n",
      "\t loss = 74958.96059930391\n",
      "\n",
      "Word 7000\n",
      "\n",
      "\t loss = 84936.59507630354\n",
      "\n",
      "Word 8000\n"
     ]
    }
   ],
   "source": [
    "w2vec = Word2Vec()\n",
    "w2vec.fit(coprusI=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "infectious-defensive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'чем'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.predict('вице')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "conventional-right",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.78828751,  0.75991898,  0.59237894,  0.28243193,  0.67849068,\n",
       "        0.8328829 ,  0.61232885,  0.99207672,  0.95076009,  0.18659157,\n",
       "        0.01499938,  0.13537804,  0.91353095,  0.91108076,  0.19866811,\n",
       "        0.34967891,  0.52696145,  0.26165538,  0.45716451,  0.69872437,\n",
       "        0.49284879,  0.7126208 ,  0.50817143, -0.0291303 ,  0.38000492,\n",
       "        0.47147017,  0.3816719 ,  0.3623444 ,  0.5015089 ,  0.40974543,\n",
       "        0.07842258,  0.25780519,  0.91474975,  0.0160981 ,  0.04048922,\n",
       "        0.27057861,  0.56805811,  0.9954855 ,  0.9617149 ,  0.99384862,\n",
       "        0.12599985,  0.67234866,  0.50216426,  0.16905834,  0.91656985,\n",
       "        0.26050474,  0.97218019,  0.58778283,  0.18946909,  0.38970682])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.emb('вице')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
